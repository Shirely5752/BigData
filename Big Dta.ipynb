{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44036640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lyu0001/anaconda3/envs/BigData/lib/python3.11/site-packages/pyspark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.find_spark_home import _find_spark_home\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel, SparkConf, SparkContext\n",
    "from pyspark.sql.functions import mean\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "print(_find_spark_home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a5eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7abffd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/21 02:16:52 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.138.133 instead (on interface ens33)\n",
      "23/12/21 02:16:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/21 02:16:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/21 02:16:54 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.app.startTime', '1703153813839')\n",
      "('spark.driver.host', '192.168.138.133')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.app.id', 'local-1703153816657')\n",
      "('spark.pyspark.python', '/home/lyu0001/anaconda3/bin/python')\n",
      "('spark.executor.memory', '12g')\n",
      "('spark.pyspark.driver.python', '/home/lyu0001/anaconda3/bin/python')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.driver.maxResultSize', '10g')\n",
      "('spark.app.submitTime', '1703153813457')\n",
      "('spark.driver.memory', '6g')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.driver.port', '45928')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.local.dir', '/home/lyu0001/spark-temp')\n",
      "('spark.app.name', 'PySpark Hadoop Integration')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n"
     ]
    }
   ],
   "source": [
    "conf = (\n",
    "    SparkConf()\n",
    "    .setMaster(\"local[*]\")\n",
    "    .set(\"spark.local.dir\", \"/home/lyu0001/spark-temp\")\n",
    "    .set(\"spark.driver.memory\", \"6g\")\n",
    "    .set(\"spark.executor.memory\", \"12g\")\n",
    "    .set(\"spark.driver.maxResultSize\", \"10g\")\n",
    "    .set(\"spark.pyspark.python\", \"/home/lyu0001/anaconda3/bin/python\")\n",
    "    .set(\"spark.pyspark.driver.python\", \"/home/lyu0001/anaconda3/bin/python\")\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySpark Hadoop Integration\")\n",
    "    .config(conf=conf)\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "\n",
    "for item in sc.getConf().getAll():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "344c9797",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+----------+--------------+-------+-----+------+------+-------+-------+----------+\n",
      "|number_sta|   lat|   lon|height_sta|          date|     dd|   ff|precip|    hu|     td|      t|       psl|\n",
      "+----------+------+------+----------+--------------+-------+-----+------+------+-------+-------+----------+\n",
      "|  14066001|49.330|-0.430|     2.000|20160101 00:00|210.000|4.400| 0.000|91.000|278.450|279.850|      NULL|\n",
      "|  14126001|49.150| 0.040|   125.000|20160101 00:00|   NULL| NULL| 0.000|99.000|278.350|278.450|      NULL|\n",
      "|  14137001|49.180|-0.460|    67.000|20160101 00:00|220.000|0.600| 0.000|92.000|276.450|277.650|102360.000|\n",
      "|  14216001|48.930|-0.150|   155.000|20160101 00:00|220.000|1.900| 0.000|95.000|278.250|278.950|      NULL|\n",
      "|  14296001|48.800|-1.030|   339.000|20160101 00:00|   NULL| NULL| 0.000|  NULL|   NULL|278.350|      NULL|\n",
      "|  14357002|48.930|-0.690|   223.000|20160101 00:00|   NULL| NULL| 0.000|  NULL|   NULL|277.650|      NULL|\n",
      "|  14366002|49.170| 0.230|    62.000|20160101 00:00|   NULL| NULL| 0.000|  NULL|   NULL|279.550|      NULL|\n",
      "|  14372001|49.102|-0.765|   184.000|20160101 00:00|230.000|4.100| 0.000|92.000|278.050|279.250|      NULL|\n",
      "|  14501002|48.890|-0.390|   185.000|20160101 00:00|   NULL| NULL| 0.000|  NULL|   NULL|278.350|      NULL|\n",
      "|  14515001|49.350|-0.770|    68.000|20160101 00:00|220.000|5.100|  NULL|  NULL|   NULL|   NULL|      NULL|\n",
      "|  14577003|49.280|-0.560|    15.000|20160101 00:00|   NULL| NULL| 0.000|99.000|279.650|279.750|      NULL|\n",
      "|  14578001|49.360| 0.170|   143.000|20160101 00:00|230.000|2.500| 0.000|96.000|277.450|278.050|102380.000|\n",
      "|  14624001|48.990|-0.010|    52.000|20160101 00:00|   NULL| NULL| 0.000|  NULL|   NULL|278.050|      NULL|\n",
      "|  14659001|49.060|-0.230|    62.000|20160101 00:00|   NULL| NULL| 0.000|  NULL|   NULL|279.150|      NULL|\n",
      "|  14762004|48.850|-0.900|   100.000|20160101 00:00|170.000|1.000| 0.000|91.000|276.550|277.850|      NULL|\n",
      "|  17218001|46.315|-1.000|     2.000|20160101 00:00|   NULL| NULL| 0.000|  NULL|   NULL|280.050|      NULL|\n",
      "|  22016001|48.857|-3.005|    25.000|20160101 00:00|   NULL| NULL| 0.000|  NULL|   NULL|278.550|      NULL|\n",
      "|  22092001|48.400|-3.150|   281.000|20160101 00:00|160.000|2.300| 0.000|97.000|277.050|277.450|      NULL|\n",
      "|  22113006|48.760|-3.470|    87.000|20160101 00:00|150.000|2.700| 0.000|97.000|275.950|276.350|102220.000|\n",
      "|  22135001|48.550|-3.380|   148.000|20160101 00:00|170.000|1.200| 0.000|97.000|275.050|275.450|      NULL|\n",
      "+----------+------+------+----------+--------------+-------+-----+------+------+-------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#展示数据\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"hdfs://localhost:9000/datasets/WeatherForcast\")\n",
    "\n",
    "type(df)  #告诉你现在df是pyspark dataframe而不是我们平常用的pandas dataframe，如果你想把现在这个pys-df转为平常用的pan-df，就相当于是下载到本地，太过占用内存，所以不要！\n",
    "#pyspark dataframe本来就是用来处理数据量比较大的数据的，pan-df只适用于小型数据\n",
    "\n",
    "df_copy = df.alias(\"df_copy\") #为了防止原始数据被覆盖\n",
    "\n",
    "#df.show()\n",
    "df_copy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff51ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('number_sta', 'string'), ('lat', 'string'), ('lon', 'string'), ('height_sta', 'string'), ('date', 'string'), ('dd', 'string'), ('ff', 'string'), ('precip', 'string'), ('hu', 'string'), ('td', 'string'), ('t', 'string'), ('psl', 'string')]\n"
     ]
    }
   ],
   "source": [
    "#打印输出每一列的数据类型，发现是string\n",
    "column_types = df.dtypes\n",
    "print(column_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6654674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+----------+----+-----+----+------+----+------+------+--------+\n",
      "| number_sta|   lat|   lon|height_sta|date|   dd|  ff|precip|  hu|    td|     t|     psl|\n",
      "+-----------+------+------+----------+----+-----+----+------+----+------+------+--------+\n",
      "|1.4066001E7| 49.33| -0.43|       2.0|NULL|210.0| 4.4|   0.0|91.0|278.45|279.85|    NULL|\n",
      "|1.4126001E7| 49.15|  0.04|     125.0|NULL| NULL|NULL|   0.0|99.0|278.35|278.45|    NULL|\n",
      "|1.4137001E7| 49.18| -0.46|      67.0|NULL|220.0| 0.6|   0.0|92.0|276.45|277.65|102360.0|\n",
      "|1.4216001E7| 48.93| -0.15|     155.0|NULL|220.0| 1.9|   0.0|95.0|278.25|278.95|    NULL|\n",
      "|1.4296001E7|  48.8| -1.03|     339.0|NULL| NULL|NULL|   0.0|NULL|  NULL|278.35|    NULL|\n",
      "|1.4357002E7| 48.93| -0.69|     223.0|NULL| NULL|NULL|   0.0|NULL|  NULL|277.65|    NULL|\n",
      "|1.4366002E7| 49.17|  0.23|      62.0|NULL| NULL|NULL|   0.0|NULL|  NULL|279.55|    NULL|\n",
      "|1.4372001E7|49.102|-0.765|     184.0|NULL|230.0| 4.1|   0.0|92.0|278.05|279.25|    NULL|\n",
      "|1.4501002E7| 48.89| -0.39|     185.0|NULL| NULL|NULL|   0.0|NULL|  NULL|278.35|    NULL|\n",
      "|1.4515001E7| 49.35| -0.77|      68.0|NULL|220.0| 5.1|  NULL|NULL|  NULL|  NULL|    NULL|\n",
      "|1.4577003E7| 49.28| -0.56|      15.0|NULL| NULL|NULL|   0.0|99.0|279.65|279.75|    NULL|\n",
      "|1.4578001E7| 49.36|  0.17|     143.0|NULL|230.0| 2.5|   0.0|96.0|277.45|278.05|102380.0|\n",
      "|1.4624001E7| 48.99| -0.01|      52.0|NULL| NULL|NULL|   0.0|NULL|  NULL|278.05|    NULL|\n",
      "|1.4659001E7| 49.06| -0.23|      62.0|NULL| NULL|NULL|   0.0|NULL|  NULL|279.15|    NULL|\n",
      "|1.4762004E7| 48.85|  -0.9|     100.0|NULL|170.0| 1.0|   0.0|91.0|276.55|277.85|    NULL|\n",
      "|   1.7218E7|46.315|  -1.0|       2.0|NULL| NULL|NULL|   0.0|NULL|  NULL|280.05|    NULL|\n",
      "|   2.2016E7|48.857|-3.005|      25.0|NULL| NULL|NULL|   0.0|NULL|  NULL|278.55|    NULL|\n",
      "|   2.2092E7|  48.4| -3.15|     281.0|NULL|160.0| 2.3|   0.0|97.0|277.05|277.45|    NULL|\n",
      "|2.2113006E7| 48.76| -3.47|      87.0|NULL|150.0| 2.7|   0.0|97.0|275.95|276.35|102220.0|\n",
      "|   2.2135E7| 48.55| -3.38|     148.0|NULL|170.0| 1.2|   0.0|97.0|275.05|275.45|    NULL|\n",
      "+-----------+------+------+----------+----+-----+----+------+----+------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#将df里所有列从string转为数值类型（浮点类型、整数类型）的数据（因为字符串不能算均值）\n",
    "\n",
    "all_columns = df.columns\n",
    "\n",
    "# 循环遍历每列并将其转换为数值类型\n",
    "for column in all_columns:\n",
    "    df = df.withColumn(column, df[column].cast(FloatType()))  # 使用 FloatType() 作为示例，你可以根据需要选择其他类型\n",
    "\n",
    "# 打印结果\n",
    "df.show() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155eabe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece477c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+----------+----+---------+---------+-----------+---------+---------+--------+---------+\n",
      "| number_sta|   lat|   lon|height_sta|date|       dd|       ff|     precip|       hu|       td|       t|      psl|\n",
      "+-----------+------+------+----------+----+---------+---------+-----------+---------+---------+--------+---------+\n",
      "|1.4066001E7| 49.33| -0.43|       2.0|NULL|    210.0|      4.4|        0.0|     91.0|   278.45|  279.85|101738.58|\n",
      "|1.4126001E7| 49.15|  0.04|     125.0|NULL|186.64313|3.8138418|        0.0|     99.0|   278.35|  278.45|101738.58|\n",
      "|1.4137001E7| 49.18| -0.46|      67.0|NULL|    220.0|      0.6|        0.0|     92.0|   276.45|  277.65| 102360.0|\n",
      "|1.4216001E7| 48.93| -0.15|     155.0|NULL|    220.0|      1.9|        0.0|     95.0|   278.25|  278.95|101738.58|\n",
      "|1.4296001E7|  48.8| -1.03|     339.0|NULL|186.64313|3.8138418|        0.0|80.639915|281.39838|  278.35|101738.58|\n",
      "|1.4357002E7| 48.93| -0.69|     223.0|NULL|186.64313|3.8138418|        0.0|80.639915|281.39838|  277.65|101738.58|\n",
      "|1.4366002E7| 49.17|  0.23|      62.0|NULL|186.64313|3.8138418|        0.0|80.639915|281.39838|  279.55|101738.58|\n",
      "|1.4372001E7|49.102|-0.765|     184.0|NULL|    230.0|      4.1|        0.0|     92.0|   278.05|  279.25|101738.58|\n",
      "|1.4501002E7| 48.89| -0.39|     185.0|NULL|186.64313|3.8138418|        0.0|80.639915|281.39838|  278.35|101738.58|\n",
      "|1.4515001E7| 49.35| -0.77|      68.0|NULL|    220.0|      5.1|0.008400894|80.639915|281.39838|285.0512|101738.58|\n",
      "|1.4577003E7| 49.28| -0.56|      15.0|NULL|186.64313|3.8138418|        0.0|     99.0|   279.65|  279.75|101738.58|\n",
      "|1.4578001E7| 49.36|  0.17|     143.0|NULL|    230.0|      2.5|        0.0|     96.0|   277.45|  278.05| 102380.0|\n",
      "|1.4624001E7| 48.99| -0.01|      52.0|NULL|186.64313|3.8138418|        0.0|80.639915|281.39838|  278.05|101738.58|\n",
      "|1.4659001E7| 49.06| -0.23|      62.0|NULL|186.64313|3.8138418|        0.0|80.639915|281.39838|  279.15|101738.58|\n",
      "|1.4762004E7| 48.85|  -0.9|     100.0|NULL|    170.0|      1.0|        0.0|     91.0|   276.55|  277.85|101738.58|\n",
      "|   1.7218E7|46.315|  -1.0|       2.0|NULL|186.64313|3.8138418|        0.0|80.639915|281.39838|  280.05|101738.58|\n",
      "|   2.2016E7|48.857|-3.005|      25.0|NULL|186.64313|3.8138418|        0.0|80.639915|281.39838|  278.55|101738.58|\n",
      "|   2.2092E7|  48.4| -3.15|     281.0|NULL|    160.0|      2.3|        0.0|     97.0|   277.05|  277.45|101738.58|\n",
      "|2.2113006E7| 48.76| -3.47|      87.0|NULL|    150.0|      2.7|        0.0|     97.0|   275.95|  276.35| 102220.0|\n",
      "|   2.2135E7| 48.55| -3.38|     148.0|NULL|    170.0|      1.2|        0.0|     97.0|   275.05|  275.45|101738.58|\n",
      "+-----------+------+------+----------+----+---------+---------+-----------+---------+---------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#==============================1.mean value（Imputer）==============#\n",
    "\n",
    "columns_to_fill = [\"dd\", \"ff\", \"precip\", \"hu\", \"td\", \"t\", \"psl\"] #需要被填充的列名\n",
    "\n",
    "# 创建 Imputer 实例并设置输入输出列\n",
    "imputer = Imputer(inputCols=columns_to_fill, outputCols=[f\"{col}\" for col in columns_to_fill])\n",
    "\n",
    "# 训练 Imputer 模型\n",
    "imputer_model = imputer.fit(df)\n",
    "\n",
    "# 应用模型填充缺失值\n",
    "df_imputed = imputer_model.transform(df)\n",
    "\n",
    "# 打印结果\n",
    "df_imputed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46adfa70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    df = df.withColumn(column, col(column).cast(\"string\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_column = df_copy.select(\"number_sta\")\n",
    "df = df.withColumn(\"number_sta\", lit(selected_column.first()[\"number_sta\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b89a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_column = df_copy.select(\"date\")\n",
    "df = df.withColumn(\"date\", lit(selected_column.first()[\"date\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================2.linear regression=======================================#\n",
    "\n",
    "# columns_to_fill = [\"dd\", \"ff\", \"precip\", \"hu\", \"td\", \"t\", \"psl\"]\n",
    "\n",
    "# # # 创建一个 VectorAssembler 将特征列合并为一个特征向量列\n",
    "# # feature_columns = [\"number_sta\"]  # 选择作为特征的列\n",
    "# # assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# # # 训练一个线性回归模型，用于填充每个缺失值列\n",
    "# # for column_to_fill in columns_to_fill:\n",
    "# #     # 过滤出不含缺失值的数据用于训练\n",
    "# #     training_data = df.filter(col(column_to_fill).isNotNull())\n",
    "\n",
    "# #     # 创建特征向量\n",
    "# #     df_assembled = assembler.transform(training_data)\n",
    "\n",
    "# #     # 创建线性回归模型\n",
    "# #     lr = LinearRegression(featuresCol=\"features\", labelCol=column_to_fill, predictionCol=f\"{column_to_fill}_predicted\")\n",
    "\n",
    "# #     # 训练模型\n",
    "# #     model = lr.fit(df_assembled)\n",
    "\n",
    "# #     # 使用模型预测缺失值列\n",
    "# #     df_predicted = assembler.transform(df.filter(col(column_to_fill).isNull()))\n",
    "# #     df_predicted = model.transform(df_predicted)\n",
    "\n",
    "# #     # 将预测结果填充回原始 DataFrame\n",
    "# #     df = df.join(df_predicted.select(\"number_sta\", f\"{column_to_fill}_predicted\"), \"number_sta\", \"left_outer\")\n",
    "# #     df = df.withColumn(column_to_fill, col(f\"{column_to_fill}_predicted\").cast(df[column_to_fill].dataType))\n",
    "\n",
    "\n",
    "# df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
